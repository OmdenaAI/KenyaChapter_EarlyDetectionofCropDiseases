{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1JMJtMKcQ5yAkbSJWl7llNPXF1kZczsz6",
      "authorship_tag": "ABX9TyOB1ng7tjsPKNF0YJZUDLiZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OmdenaAI/KenyaChapter_EarlyDetectionofCropDiseases/blob/main/VITmodel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
        "from tqdm import tqdm\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "lA8vTqsq9MIl"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IZr06vH9PGg",
        "outputId": "b2b1b7a6-c257-4c07-d5f3-41722b2e044a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "batch_size = 16  # Adjust based on your GPU memory\n",
        "num_epochs = 10   # Reduce for quick testing\n",
        "learning_rate = 5e-5  # Typical learning rate for ViT fine-tuning\n",
        "image_size = 224  # Resize all images to 224x224\n",
        "num_classes = 13  # Number of classes (diseases + healthy)"
      ],
      "metadata": {
        "id": "CwcjQV4k9SuG"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class labels (ensure this matches your dataset)\n",
        "class_labels = [\n",
        "    'Bacterial spot', 'Black mold', 'Gray spot', 'Late blight',\n",
        "    'Tomato___Early_blight', 'Tomato___Late_blight', 'Tomato___Leaf_Mold',\n",
        "    'Tomato___Septoria_leaf_spot', 'Tomato___Target_Spot',\n",
        "    'Tomato___Tomato_Yellow_Leaf_Curl_Virus', 'Tomato___Tomato_mosaic_virus',\n",
        "    'Tomato___healthy', 'healthy'\n",
        "]"
      ],
      "metadata": {
        "id": "6_jinhlU9Vxx"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations for the dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),  # Resize all images\n",
        "    transforms.ToTensor(),  # Convert to Tensor\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize\n",
        "])"
      ],
      "metadata": {
        "id": "iW-dZiF59YXi"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset using ImageFolder\n",
        "dataset_path = \"/content/drive/MyDrive/tomato-images\"  # Update this path\n",
        "dataset = datasets.ImageFolder(root=dataset_path, transform=transform)"
      ],
      "metadata": {
        "id": "8gePd5V99bDw"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset into training and validation sets (80% train, 20% val)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"
      ],
      "metadata": {
        "id": "tLSHPlAX9hG0"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoaders for training and validation\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "id": "XqlKcRSC9l4H"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pretrained ViT model and modify for classification\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    \"google/vit-base-patch16-224-in21k\",\n",
        "    num_labels=num_classes\n",
        ")\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cra1315r9qlC",
        "outputId": "0bbb9352-85f3-450f-e962-201efb90cfde"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViTForImageClassification(\n",
              "  (vit): ViTModel(\n",
              "    (embeddings): ViTEmbeddings(\n",
              "      (patch_embeddings): ViTPatchEmbeddings(\n",
              "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (encoder): ViTEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x ViTLayer(\n",
              "          (attention): ViTSdpaAttention(\n",
              "            (attention): ViTSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (output): ViTSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ViTIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): ViTOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "  )\n",
              "  (classifier): Linear(in_features=768, out_features=13, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the optimizer, loss function, and learning rate scheduler\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.8)  # Optional scheduler"
      ],
      "metadata": {
        "id": "sQqu88tq9t5q"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable mixed precision training\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "scaler = torch.amp.GradScaler('cuda')\n"
      ],
      "metadata": {
        "id": "Dm0keekr95fI"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "scaler = torch.amp.GradScaler()  # Proper initialization\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()  # Reset gradients\n",
        "\n",
        "        # Enable mixed precision\n",
        "        with torch.amp.autocast(device_type='cuda'):\n",
        "            outputs = model(pixel_values=inputs)\n",
        "            loss = criterion(outputs.logits, labels)\n",
        "\n",
        "        # Backpropagation with scaling\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "    # Step the learning rate scheduler\n",
        "    scheduler.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YezgD5eK98eG",
        "outputId": "a0646e70-d1d2-442f-8ef5-45d54814c6b9"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/10: 100%|██████████| 687/687 [02:16<00:00,  5.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss: 0.40938722554816603\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/10: 100%|██████████| 687/687 [02:01<00:00,  5.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Loss: 0.07122949768383777\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3/10: 100%|██████████| 687/687 [02:00<00:00,  5.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 Loss: 0.03842757437161266\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4/10: 100%|██████████| 687/687 [02:00<00:00,  5.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 Loss: 0.0157784520278331\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5/10: 100%|██████████| 687/687 [01:59<00:00,  5.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 Loss: 0.00897352616851702\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 6/10: 100%|██████████| 687/687 [01:59<00:00,  5.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 Loss: 0.005936562214141611\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 7/10: 100%|██████████| 687/687 [01:59<00:00,  5.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 Loss: 0.004405399217111705\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 8/10: 100%|██████████| 687/687 [02:00<00:00,  5.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 Loss: 0.0034379644198763407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 9/10: 100%|██████████| 687/687 [01:59<00:00,  5.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 Loss: 0.002766118219340548\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 10/10: 100%|██████████| 687/687 [01:59<00:00,  5.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 Loss: 0.002277648832013199\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step the learning rate scheduler\n",
        "scheduler.step()"
      ],
      "metadata": {
        "id": "o0w3b8fXDrJ6"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model and feature extractor\n",
        "model.save_pretrained(\"./vit-tomato-disease\")\n",
        "print(\"Model saved!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhxglTlcD2XU",
        "outputId": "7a4fac6c-5f92-480a-e6dc-36925c111920"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model and feature extractor\n",
        "from transformers import ViTFeatureExtractor, ViTForImageClassification\n",
        "from PIL import Image\n",
        "\n",
        "# Load model and set to evaluation mode\n",
        "model = ViTForImageClassification.from_pretrained(\"./vit-tomato-disease\").to(device)\n",
        "model.eval()\n",
        "\n",
        "# Define the feature extractor\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "\n",
        "# Define test image path\n",
        "test_image_path = \"/content/drive/MyDrive/OIP (3).jpeg\"  # Update this path\n",
        "image = Image.open(test_image_path)\n",
        "\n",
        "# Preprocess the image\n",
        "inputs = feature_extractor(images=image, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Make prediction\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    predicted_label_index = outputs.logits.argmax(-1).item()\n",
        "\n",
        "# Map the prediction to the disease name\n",
        "predicted_disease = class_labels[predicted_label_index]\n",
        "print(f\"Predicted Disease: {predicted_disease}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JLHFmbUD8n4",
        "outputId": "7dc1c43a-f59d-4a3a-89b9-37e0288f3aec"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Disease: Black mold\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Evaluate model on validation dataset\n",
        "def evaluate_model(model, val_loader):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(val_loader, desc=\"Evaluating\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Make predictions\n",
        "            outputs = model(pixel_values=inputs)\n",
        "            preds = outputs.logits.argmax(dim=1)  # Get predicted class indices\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Compute accuracy\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    return accuracy\n",
        "\n",
        "# Call the evaluation function and print accuracy\n",
        "val_accuracy = evaluate_model(model, val_loader)\n",
        "print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qhE_tliEvfX",
        "outputId": "2e3e38b5-6390-4092-aebd-bba864a8d97e"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 172/172 [00:31<00:00,  5.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 99.45%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}